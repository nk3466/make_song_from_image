{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c60abdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reference:\n",
    "- ## https://velog.io/@yeah7598/KoGPT2-%EB%8F%99%ED%99%94-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0\n",
    "- ## https://github.com/ttop32/KoGPT2novel\n",
    "- ## inference: https://github.com/ttop32/KoGPT2novel/blob/main/train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8314e386",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\gpu2.6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Free allocated memory (for CUDA out of memeory error)\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e704ce49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers \n",
    "from transformers import PreTrainedTokenizerFast\n",
    "# from transformers import AutoModelWithLMHead # ì´ ë¼ì¸ì´ í•„ìš”í•œì§€ í™•ì¸\n",
    "from fastai.text.all import *\n",
    "import fastai\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import GPT2LMHeadModel # Using only GPT2LM Head Model\n",
    "from transformers import PreTrainedTokenizerFast # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49522b4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•',\n",
       " 'í•˜',\n",
       " 'ì„¸',\n",
       " 'ìš”.',\n",
       " 'â–í•œêµ­ì–´',\n",
       " 'â–G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " 'â–ì…',\n",
       " 'ë‹ˆë‹¤.',\n",
       " 'ğŸ˜¤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f636c9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"kogpt_model_finetunned_all\") # GPT2LMHeadModel/ AutoModelWithLMHead í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b203f0f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì°¨ë¥¼ íƒ€ê³  ì—¬í–‰ì„ ê°€ìš”ì—†ì–´ í˜ë“¤ë©´ë¼ ì—¬ê¸°ë„í•´ì•¼í• ë•ŒëŠ”ê³³ìœ¼ë¡œ ê¸°ì°¨ë¥¼íƒ€ê³  ì´ê³³ì €ê¸°ì°¨ì—¬í–‰ê¸¸ í•œë°¤ ë­‰í´í•œë°”ëŒì´ë‚ ì•„ë¦„ë‹µê²Œ ì €ë…ì—”ì˜¤ì§íˆë°•íŒë§˜ì†ê¹Šê²Œì™„ë²½í•˜ê²Œì—‰ë§ì„¤ë ˆëŠ”ë‚´ì˜†ì—ìˆì–´ ê·¸ë˜ì–¸ì œì¶œë°œê±¸ìŒë‘ê·¼ì‹¬ìŠ¤ë ˆë‹¤ê°€ì™€í’€ ì‚¬ì´ë¡œë©”ê³ ë¯¼ê±°ë¦¬ëŠ”ëª°ë¼ ì´ê¸¸ìˆ˜ìˆê²Œ ì–´ìƒ‰í•œì¸ë“¯í•œìŠ¤ì³ê°€ë“ë– ë‚˜ì˜¨ëŠë‚Œì„ê°ì‹¸ë˜ê·¸ë§Œë‚˜ê²Œë í…ë° ë–¨ë¦¬ëŠ”ì™¸ë¡œìš´ì„¸ìƒê´€ë¬¸ì—ì•ˆì˜¬ë ¤ì›€ì¯¤ì€ëª¨ë“ ìˆœê°„ì•Œì•˜ê¸°ì— ë‚˜ëŠ”ì§€ê¸ˆ\n"
     ]
    }
   ],
   "source": [
    "# Inference with model\n",
    "\n",
    "text = \"\"\"ì°¨ë¥¼ íƒ€ê³  ì—¬í–‰ì„ ê°€ìš”\"\"\"\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True\n",
    "                        )\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55ddac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
